{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Model Documentation <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lightweight library to create self-contained executable models with focus on compatibility, simplicity, and fast experimentation.\n",
    "\n",
    "At its most basic, a trained model is just a function that takes some input and produces some output. This library builds on this assumption and enables you to package up the preprocessing & prediction logic, and all of your model artifacts into a single model archive file that you can then easily share, distribute, and deploy. There is no need for configuration files, copying files on the file system, or other manual tasks. You can define and save your model without leaving your python notebook or script. With the unified model format, you can build your model once and run it anywhere. It provides a huge flexibility to deploy and serve your models in any environment. Furthermore, this model format makes it possible to easily combine (e.g. via voting ensembles) and evaluate models without having to know the underlying machine learning library.\n",
    "\n",
    "**Key Aspects:**\n",
    "* Build once, run anywhere.\n",
    "* Works with any machine learning library that comes with a python interface.\n",
    "* Packages all your model logic, requirements, and artifacts into a single self-container file.\n",
    "\n",
    "This tutorial shows you everything you need to know in order to use this library for your machine learning models. \n",
    "\n",
    "**In this notebook:**\n",
    "\n",
    "* Unified Model Basics: Define, Use, Save, Load, Deploy, ...\n",
    "* Case Study - Text Classification: Unified Models in Action\n",
    "\n",
    "_The library and this notebook is only tested with Python 3._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "In the first step, we will just install and import all dependencies required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:22.903570Z",
     "start_time": "2018-11-19T13:08:19.587522Z"
    }
   },
   "outputs": [],
   "source": [
    "# System libraries.\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import os, logging, sys, inspect\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "# Intialize tqdm to always use the notebook progress bar\n",
    "import tqdm\n",
    "tqdm.tqdm = tqdm.tqdm_notebook\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(format='[%(levelname)s] %(message)s', level=logging.INFO, stream=sys.stdout)\n",
    "\n",
    "# Lab libraries\n",
    "from lab_client import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:24.022797Z",
     "start_time": "2018-11-19T13:08:22.907277Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = Environment() \n",
    "\n",
    "# Create experiment\n",
    "exp = env.create_experiment('Unified Model Tutorial')\n",
    "output_path = exp.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Model - Basics\n",
    "---\n",
    "\n",
    "**Required Time:** ~10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "Define your first unified model. The minimum requirement is that you overwrite the `_predict` function. This function is expected to return predictions from the model based on the provided `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:24.047773Z",
     "start_time": "2018-11-19T13:08:24.025385Z"
    }
   },
   "outputs": [],
   "source": [
    "from unified_model import UnifiedModel\n",
    "\n",
    "# This basic model just returns the data that it gets in.\n",
    "class MyEchoModel(UnifiedModel):\n",
    "    def _predict(self, data, **kwargs):\n",
    "        # Implement this function with the logic to make a prediction on the data item. \n",
    "        # In this example we just return the data itself.\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Use Model\n",
    "To use the model, you need to create a new instance and call the `predict` method with a data item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:24.070428Z",
     "start_time": "2018-11-19T13:08:24.054310Z"
    }
   },
   "outputs": [],
   "source": [
    "echo_model = MyEchoModel()\n",
    "echo_model.predict(\"This is an data example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "To make the model portable, you can save the model as a single file to a given path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:24.098538Z",
     "start_time": "2018-11-19T13:08:24.078180Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = echo_model.save(os.path.join(output_path,'my_first_model.model.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The saved model is a zip file that contains the python pickle of your model and a few other artifacts in a specific structure (more details in [\"A Look inside the Model\"](#A-Look-inside-the-Model)). You can unzip the model with any unzipping tool. You can also compress the model via the `compress` parameter (this is deactivated on default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "You can load any unified model file via `UnifiedModel.load(model_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:24.127642Z",
     "start_time": "2018-11-19T13:08:24.100951Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_model = UnifiedModel.load(model_path)\n",
    "loaded_model.predict(\"This is an data example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can also load a unified model from a folder via `UnifiedModel.load(model_path)` as well, if it has the right folder structure as explained in [\"A Look inside the Model\"](#A-Look-inside-the-Model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metadata\n",
    "Adding metadata to the model allows you to provide information on how the model was trained, which data was used, how the data is preprocessed, the type of the model and more. You can save those additional metadata to the model by providing a dictonary at model initialization via the `info_dict` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:24.134986Z",
     "start_time": "2018-11-19T13:08:24.130346Z"
    }
   },
   "outputs": [],
   "source": [
    "model_info = {\n",
    "    \"description\": \"Returns the data that it gets in.\"\n",
    "}\n",
    "\n",
    "echo_model = MyEchoModel(info_dict=model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `info()` function to get this metadata about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:24.151920Z",
     "start_time": "2018-11-19T13:08:24.138265Z"
    }
   },
   "outputs": [],
   "source": [
    "echo_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model also has a name which you can change on model initalization via the `name` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:24.159717Z",
     "start_time": "2018-11-19T13:08:24.154974Z"
    }
   },
   "outputs": [],
   "source": [
    "print(str(echo_model))\n",
    "echo_model = MyEchoModel(name=\"echo_model\")\n",
    "print(str(echo_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Lifecycle\n",
    "The Unified Model also provides lifecycle methods that can be overwritten if additional processing is required for model save (`_save_model(output_path)`) and load (`_init_model()`). See the comments below for additonal details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:24.195333Z",
     "start_time": "2018-11-19T13:08:24.162591Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyEchoModel(UnifiedModel):\n",
    "    def _init_model(self):\n",
    "        # Called after the model is unpickled. \n",
    "        # Overwrite this method if additional initialization is required.\n",
    "        print(\"init model\")\n",
    "    \n",
    "    def _save_model(self, output_path):\n",
    "        # Called before the model is saved to the file system. \n",
    "        # Overwrite this method if additional processing is required before the model is saved.\n",
    "        print(\"save model\")\n",
    "    \n",
    "    def _predict(self, data, **kwargs):\n",
    "        # Implement this function with the logic to make a prediction on the data item. \n",
    "        # In this example we just return the text itself.\n",
    "        return data\n",
    "    \n",
    "echo_model = MyEchoModel()\n",
    "\n",
    "# The save function will call _save_model() before saving the model to the filesystem, \n",
    "# and _init_model after it is saved to initialize the model again for direct usage.\n",
    "print(\"Saving Model:\")\n",
    "model_path = echo_model.save(os.path.join(output_path,'my_first_model.model.zip'))\n",
    "\n",
    "# The load function will only call _init_model() after the unpickeling\n",
    "print(\"Loading Model:\")\n",
    "UnifiedModel.load(model_path).predict(\"This is an data example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Requirements\n",
    "In most cases, you will have third-party dependencies in your model. You can add those dependencies to the model via `add_requirements()` or at model initialization with the `requirements` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add dependencies, you can either add pip-installable libraries or imported modules. Imported modules will be packaged with the unified model with all source code of the given library/modul when the model is saved. If possible, we suggest to add dependencies via the pip library instead of the imported module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:27.619844Z",
     "start_time": "2018-11-19T13:08:24.198616Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:30.762050Z",
     "start_time": "2018-11-19T13:08:27.623924Z"
    }
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "class LanguageDetectionModel(UnifiedModel):\n",
    "    REQUIREMENTS = {\n",
    "        # Add all (pip) dependencies or required modules here.\n",
    "        \"pandas\", # pip installable dependency. \n",
    "        # Version requirements for pip depencies are supported as well, e.g. pandas>=0.23.0\n",
    "        detect # imported module, \n",
    "        # Modules will be packaged with the unified model with all related source code\n",
    "    }\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(LanguageDetectionModel, self).__init__(**kwargs)\n",
    "        self.add_requirements(self.REQUIREMENTS)\n",
    "            \n",
    "    def _predict(self, data, **kwargs):\n",
    "        # call langauge detection library\n",
    "        prediction = detect(str(data))\n",
    "        # add prediction result into an dataframe\n",
    "        return pd.DataFrame([prediction], columns=[\"language\"])\n",
    "\n",
    "lang_detection_model = LanguageDetectionModel()\n",
    "lang_detection_model_path = lang_detection_model.save(os.path.join(output_path,'language_detection_model.model.zip'))\n",
    "\n",
    "# To install the requirements prior to the model initalization: install_requirements=true\n",
    "# As a default it is deactivated, but activated when the model is loaded to serving.\n",
    "UnifiedModel.load(lang_detection_model_path, install_requirements=True).predict(\"This is an data example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Additionally, you can also provide a script at model initialization via the <b>setup_script</b> parameter which will be executed via /bin/sh prior to model initialization. However, we suggest to only use this option if there is no other solution. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Types\n",
    "\n",
    "\n",
    "In addition to the generic `UnifiedModel` class, you can also extend your custom model from a variety of more specific model types. The goal of model types is to have common input/output data formats and usage parameters defined based on the given machine learning task, such as image/text classification, regression, recommendation, object detection, sequence to sequence modeling, and any other tasks that can share common formats. Thereby, the model types can enforce/validate specific input and output data of the predict function, provide default data transformation, add additional prediction parameters, and other tasks to ensure a common usage for the given machine learning task. Here are a few examples of available model types:\n",
    "* **RecommendationModel:** Input= any data; Output= dataframe with atleast item and score column.\n",
    "* **ClassificationModel:** Input= same as RecommendationModel; Output= same as RecommendationModel\n",
    "* **TextClassificationModel:** Input= plain text (string); Output= same as ClassificationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see all currently provided model types with the following code. Additional model types will be added soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:08:30.776523Z",
     "start_time": "2018-11-19T13:08:30.765276Z"
    }
   },
   "outputs": [],
   "source": [
    "from unified_model import model_types\n",
    "inspect.getmembers(model_types, inspect.isclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Files to Model\n",
    "You can add arbitrary files to the model that will be packaged within the model file on save. You can add files via `add_file(key, file_path)` and get a file (path to the file) by the key with `get_file(key)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are building a unified model that is able to suggest similar words to a given input word. Therefore, we will use a pretrained word2vec model that we will download in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:09:06.496805Z",
     "start_time": "2018-11-19T13:08:30.779218Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "pretrained_wv_path = os.path.join(output_path,'pretrained_word_embeddings.bin')\n",
    "downloader.load(\"glove-wiki-gigaword-50\").save_word2vec_format(pretrained_wv_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define a `WordSuggestionModel`. Here, we will use gensim to load the pretrained word vectors and predict similar words. The downloaded file (pretrained word vectors) will be added to the model via `add_file()` and requested again in `_init_model()` via `get_file()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:09:25.938737Z",
     "start_time": "2018-11-19T13:09:06.501580Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class WordSuggestionModel(model_types.RecommendationModel):\n",
    "    W2V_MODEL = \"w2v_model.bin\"\n",
    "        \n",
    "    REQUIREMENTS = {\n",
    "        \"gensim\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self, wv_model_path, **kwargs):\n",
    "        super(WordSuggestionModel, self).__init__(**kwargs)\n",
    "        self.add_requirements(self.REQUIREMENTS)\n",
    "        # Add the file with an identifier to the model. \n",
    "        # The file will be bundled into the model file on model save.\n",
    "        self.add_file(self.W2V_MODEL,wv_model_path)\n",
    "        # Call the model initialization\n",
    "        self._init_model()\n",
    "    \n",
    "    def _init_model(self):\n",
    "        # get the added file from the model and load it. \n",
    "        self.gensim_wv_model = KeyedVectors.load_word2vec_format(\n",
    "            self.get_file(self.W2V_MODEL), binary=True)\n",
    "    \n",
    "    def _save_model(self, output_path):\n",
    "        # Delete the word vectors instance, so that it is not included into the pickle.\n",
    "        # The added file will be automatically packaged with the model.\n",
    "        del self.gensim_wv_model\n",
    "            \n",
    "    def _predict(self, data, limit=5, **kwargs):\n",
    "        most_similar = self.gensim_wv_model.similar_by_word(str(data), limit)\n",
    "        return pd.DataFrame(most_similar, columns=[\"item\", \"score\"])\n",
    "    \n",
    "word_suggestions_model = WordSuggestionModel(pretrained_wv_path)\n",
    "word_suggestions_model_path = word_suggestions_model.save(\n",
    "    os.path.join(output_path,'word_suggestion_model.model.zip'))\n",
    "\n",
    "UnifiedModel.load(word_suggestions_model_path).predict(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the above example, we also could just pickle the gensim wordvectors instance instead of storing and loading it as a file. However, their are many cases were pickle fails to serialize certain objects or were you would like to preserve the orginial file formats within the unified model package. In those cases, you have to use the add/get file functionality. To test whether a model can be successfully loaded in another python enviornment, we provide test utilities as explained in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Look inside the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file is a zip file that contains the python pickle of your model and a few other artifacts in a specific structure. The model files is a valid Python Zip Application as defined in [PEP 441](https://www.python.org/dev/peps/pep-0441) and, therefore, can be direcly executed. The unified model has the following structure and artifacts which are automatically created during `save()`: \n",
    "* **info.json**                  _-> Export of the model metadata (info dictionary)._\n",
    "* **\\_\\_main\\_\\_.py**                _-> Starts the CLI interface for serving and prediction of the model._\n",
    "* **data/**                      _-> All the data required for the initalization of the model._\n",
    "  * **unified_model.pkl**        _-> Pickle file of the model instance that contains all the model's data processing and prediction logic._\n",
    "* **code/**                      _-> All modules and libraries in this directory are added to the python path prior to model initialization. Added modules will be stored here._\n",
    "* **requirements.txt**           _-> Optional: List of requirements installed via pip prior to model initialization_\n",
    "* **setup.sh**                   _-> Optional: Setup script that is executed prior to model initialization_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to look inside the `WordSuggestionModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:09:25.949563Z",
     "start_time": "2018-11-19T13:09:25.941678Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zipfile.ZipFile(word_suggestions_model_path, 'r').namelist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined Models\n",
    "For many common machine learning libraries, we will provide predefined model wrappers for different model types so that you are not required to always define you own custom models. \n",
    "\n",
    "* [FastText](https://github.com/facebookresearch/fastText):\n",
    "    * **FasttextClassifier** (TextClassificationModel): Can be initialized with any fasttext classification model file via the `ft_classifier_path` parameter.\n",
    "* [Sklearn](https://github.com/scikit-learn/scikit-learn):\n",
    "    * **SklearnTextClassifier** (TextClassificationModel): Can be initialized with any sklearn classifier via the `sklearn_classifier` parameter.\n",
    "\n",
    "Additional predefined model wrappers will be added soon. You can see some predefined models in action in the [\"Case Study - Text Classification\"](#Case-Study---Text-Classification) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add your own data preprocessing logic to a predefined model, you can provide a transformation method via the `transform_func` parameter during model initialization. The transformation method should have the following format: \n",
    "``` python\n",
    "def transform(data, model=None, **kwargs):\n",
    "    # Apply your preprocessing on the data\n",
    "    return data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve & Deploy Model\n",
    "The unified model format provides a huge flexibility to deploy and serve your models in any environment. In the following section we will serve with an REST API the model via CLI as well as a Docker image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve via CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-02T11:43:07.119037Z",
     "start_time": "2018-11-02T11:41:56.262Z"
    }
   },
   "source": [
    "``` bash\n",
    "python {word_suggestions_model_path} serve --port 8060\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve via Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-16T19:58:20.034941Z",
     "start_time": "2018-11-16T19:58:20.021846Z"
    }
   },
   "source": [
    "``` bash\n",
    "docker run -d -p 8091:8091 -v {word_suggestions_model_path}:/default_model mltooling/unified-model-service:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look on the [\"Compatibility & Deployment\"](#Compatiblity-&amp;-Deployment) section for more deployment options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Model\n",
    "There are many factors, especially with pickling and the requirements, that prevents the model from successfully loading in another python environment. Therefore, we provide the`test_unified_model(model_instance, test_data_item=None)` functionality that helps to test whether your model instance can be successfully loaded in another python environment. This utilty saves the model instance, loads the model file in another python process, and (optionally) calls `predict()` with the provided test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:09:42.549369Z",
     "start_time": "2018-11-19T13:09:27.254398Z"
    }
   },
   "outputs": [],
   "source": [
    "from unified_model.evaluation_utils import test_unified_model\n",
    "test_unified_model(word_suggestions_model,\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study - Text Classification\n",
    "---\n",
    "\n",
    "In the follwing case study, we will train a fasttext and a sklearn text classification model on the [20-newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset, and wrap the trained models into unified models. In additon, we will combine both models into an voting ensemble and apply extensive evaluation on all models.\n",
    "\n",
    "**Required Time:** ~5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "In the first step, we load the [20-newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset and split it into train, validate, and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:09:45.103291Z",
     "start_time": "2018-11-19T13:09:42.556510Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import downloader\n",
    "\n",
    "# Load newsgroups dataset: Collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.\n",
    "DATASET_NAME = \"20-newsgroups\"\n",
    "df = pd.DataFrame([data for data in downloader.load(DATASET_NAME)])\n",
    "\n",
    "# Split this dataset into train (60%), validate (20%), and test (20%)\n",
    "train_df, validate_df, test_df = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "For the sklearn classifier, we use TF-IDF to vectorize the text data and an Linear SVM as the classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:09:55.935737Z",
     "start_time": "2018-11-19T13:09:45.106369Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Preprocessing logic\n",
    "def preprocess(data, **kwargs):\n",
    "    return data.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "\n",
    "# Training logic\n",
    "def train(config):\n",
    "    global sklearn_classifier\n",
    "    \n",
    "    # Experiment Implementation\n",
    "    def default_analyzer(x):\n",
    "        return x\n",
    "\n",
    "    classification_pipeline = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(analyzer=default_analyzer,min_df=config['min_df'])),\n",
    "        (\"lsvc_calib\", CalibratedClassifierCV(LinearSVC(verbose=0),method=\"isotonic\", cv=3))])\n",
    "    \n",
    "    sklearn_classifier = classification_pipeline.fit(\n",
    "        [preprocess(item).split() for item in train_df[\"data\"].tolist()], \n",
    "        train_df[\"topic\"].tolist()\n",
    "    )\n",
    "    \n",
    "    score = sklearn_classifier.score(\n",
    "        [preprocess(item).split() for item in validate_df[\"data\"].tolist()],\n",
    "        validate_df[\"topic\"].tolist()\n",
    "    )\n",
    "    \n",
    "    print(\"Model trained. Score: \"+str(score))\n",
    "\n",
    "# RUN \n",
    "# Define default parameter configuration\n",
    "config = {\n",
    "    'min_df':2\n",
    "}\n",
    "\n",
    "# Run model training\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Define Unified Model\n",
    "Define the Unified Model for the sklearn classifier. Since we want to provide a text classification model, we can use the `TextClassificationModel` model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:09:55.975676Z",
     "start_time": "2018-11-19T13:09:55.939751Z"
    }
   },
   "outputs": [],
   "source": [
    "from unified_model.model_types import TextClassificationModel\n",
    "\n",
    "class SklearnTextClassifier(TextClassificationModel):\n",
    "    \n",
    "    REQUIREMENTS = [\n",
    "        \"scikit-learn\"\n",
    "    ]\n",
    "\n",
    "    def __init__(self, sklearn_classifier, **kwargs):\n",
    "        super(SklearnTextClassifier, self).__init__(**kwargs)\n",
    "        self.add_requirements(self.REQUIREMENTS)\n",
    "        self.sklearn_classifier = sklearn_classifier\n",
    "\n",
    "    def _predict(self, data, limit=None, **kwargs):\n",
    "        if limit is None:\n",
    "            limit = len(self.sklearn_classifier.classes_)\n",
    "\n",
    "        # Also put preprocessing logic for the data directly into the model.\n",
    "        data = preprocess(str(data))\n",
    "        # Text needs to be tokenized\n",
    "        if isinstance(data, (list, np.ndarray)):\n",
    "            tokenized_text = data\n",
    "        else:\n",
    "            tokenized_text = str(data).strip().split()  # split by whitespace\n",
    "            \n",
    "        result = []\n",
    "        class_confidences = self.sklearn_classifier.predict_proba([tokenized_text])\n",
    "        for index in np.argsort(class_confidences)[:, :-limit - 1:-1][0]:\n",
    "            item = self.sklearn_classifier.classes_[index]\n",
    "            score = class_confidences[0][index]\n",
    "            result.append([str(item), score])\n",
    "\n",
    "        # The TextClassificationModel type expects a dataframe as prediction result \n",
    "        # The resulting dataframe needs to have atleast an item and score column\n",
    "        return pd.DataFrame(result, columns=[\"item\", \"score\"])\n",
    "\n",
    "sklearn_model = SklearnTextClassifier(sklearn_classifier)\n",
    "sklearn_model.predict(\"this is a data item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Use Predefined Model Wrapper\n",
    "Instead of defining your custom model, you can also just use the predefined `SklearnTextClassifier` and provide the preprocessing logic via the `transform_func` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:09:56.089465Z",
     "start_time": "2018-11-19T13:09:55.982086Z"
    }
   },
   "outputs": [],
   "source": [
    "from unified_model.predefined_models.sklearn_models import SklearnTextClassifier\n",
    "sklearn_model = SklearnTextClassifier(sklearn_classifier, transform_func=preprocess)\n",
    "sklearn_model.predict(\"this is a data item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:10:48.947048Z",
     "start_time": "2018-11-19T13:09:56.092288Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyfasttext import FastText\n",
    "import multiprocessing\n",
    "\n",
    "# Preprocessing logic\n",
    "def preprocess(data, **kwargs):\n",
    "    return data.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "\n",
    "# Create training data for fasttext\n",
    "train_data_path = os.path.join(output_path,'data.train.txt')\n",
    "test_data_path =  os.path.join(output_path,'data.test.txt')\n",
    "with open(train_data_path, 'w') as f:\n",
    "    for index, row in train_df.iterrows():\n",
    "        f.write(\"__label__\" + row[\"topic\"].strip() + ' ' +  preprocess(row[\"data\"]) + '\\n')\n",
    "                \n",
    "with open(test_data_path, 'w') as f:\n",
    "     for index, row in validate_df.iterrows():\n",
    "        f.write(\"__label__\" + row[\"topic\"].strip() + ' ' +  preprocess(row[\"data\"]) + '\\n')\n",
    "\n",
    "# Training logic\n",
    "def train(config):\n",
    "    global fasttext_classifier\n",
    "    global fasttext_classifier_path\n",
    "    \n",
    "    fasttext_classifier_path = os.path.join(output_path, DATASET_NAME+\"_classifier_ft.model\")\n",
    "    fasttext_classifier = FastText()\n",
    "    fasttext_classifier.supervised(\n",
    "        input=train_data_path, \n",
    "        output=fasttext_classifier_path, \n",
    "        thread=multiprocessing.cpu_count(),\n",
    "        pretrainedVectors='',\n",
    "        wordNgrams=config['word_ngrams'],\n",
    "        epoch=config['epochs'],\n",
    "        minCount=config['min_count'],\n",
    "        ws=config['window_size'],\n",
    "        dim=config['vector_dim'],\n",
    "        lr=config['learning_rate'],\n",
    "        lrUpdateRate=config['lr_update_rate'],\n",
    "        neg=config['negativ_sampling'],\n",
    "        t=config['sampling'],\n",
    "        bucket=0)\n",
    "    \n",
    "    # .bin is added automatically by fasttext\n",
    "    fasttext_classifier_path = fasttext_classifier_path+\".bin\" \n",
    "    \n",
    "    # fasttext_classifier.test(test_data_path) -> print test scores out on console\n",
    "    print(\"Model trained.\")\n",
    "\n",
    "# RUN \n",
    "# Define default parameter configuration\n",
    "config = {\n",
    "    'min_count':2,\n",
    "    'window_size':5,\n",
    "    'word_ngrams':1,\n",
    "    'vector_dim':100,\n",
    "    'learning_rate':0.1,\n",
    "    'lr_update_rate':100,\n",
    "    'negativ_sampling':5,\n",
    "    'sampling':0.0001,\n",
    "    'epochs':50\n",
    "}\n",
    "\n",
    "# Run model training\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Define Unified Model\n",
    "Define the Unified Model for the fasttext classifier. Since we want to provide a text classification model, we can use the `TextClassificationModel` model type. Since pickle is not able to serialize the fasttext model instance, we have to add and load the fasttext model from the original fasttext binary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:10:49.615027Z",
     "start_time": "2018-11-19T13:10:48.950074Z"
    }
   },
   "outputs": [],
   "source": [
    "from unified_model.model_types import TextClassificationModel\n",
    "\n",
    "class FasttextClassifier(TextClassificationModel):\n",
    "    FT_MODEL_KEY = \"ft_classifier.model\"\n",
    "    \n",
    "    REQUIREMENTS = [\n",
    "        \"pyfasttext\"\n",
    "    ]\n",
    "\n",
    "    def __init__(self, ft_classifier_path, **kwargs):\n",
    "        super(FasttextClassifier, self).__init__(**kwargs)\n",
    "        self.add_requirements(FasttextClassifier.REQUIREMENTS)\n",
    "\n",
    "        self.add_file(FasttextClassifier.FT_MODEL_KEY, ft_classifier_path)\n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        self.model_instance = FastText(self.get_file(FasttextClassifier.FT_MODEL_KEY))\n",
    "\n",
    "    def _save_model(self, output_path):\n",
    "        del self.model_instance\n",
    "        \n",
    "    def _predict(self, data, limit=None, **kwargs):\n",
    "        if limit is None:\n",
    "            limit = self.model_instance.nlabels\n",
    "\n",
    "        # Also put preprocessing logic for the data directly into the model.\n",
    "        preprocessd_text = preprocess(str(data))\n",
    "        \n",
    "        result = []\n",
    "        result = self.model_instance.predict_proba_single(preprocessd_text, k=limit)\n",
    "\n",
    "           # The TextClassificationModel type expects a dataframe as prediction result \n",
    "        # The resulting dataframe needs to have atleast an item and score column\n",
    "        return pd.DataFrame(result, columns=[\"item\", \"score\"])\n",
    "\n",
    "fasttext_model = FasttextClassifier(fasttext_classifier_path)\n",
    "fasttext_model.predict(\"this is a data item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Use Predefined Model Wrapper\n",
    "Instead of defining your custom model, you can also just use the predefined `FasttextClassifier` and provide the preprocessing logic via the `transform_func` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:10:50.188068Z",
     "start_time": "2018-11-19T13:10:49.617247Z"
    }
   },
   "outputs": [],
   "source": [
    "from unified_model.predefined_models.fasttext_models import FasttextClassifier\n",
    "fasttext_classifier = FasttextClassifier(fasttext_classifier_path, transform_func=preprocess)\n",
    "fasttext_classifier.predict(\"this is a data item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Models - Voting Ensemble\n",
    "This library also provides a simple way to build voting ensembles from a collection of classifaction models via the `VotingEnsemble` class. There are various voting strategies that can be applied such as relative_score, rank_averaging, rank_vote, one_vote, total_score, highest_scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:10:50.239957Z",
     "start_time": "2018-11-19T13:10:50.193065Z"
    }
   },
   "outputs": [],
   "source": [
    "from unified_model.ensemble_utils import VotingEnsemble\n",
    "\n",
    "voting_ensemble = VotingEnsemble(models=[sklearn_model,\n",
    "                                        fasttext_model], strategy=\"relative_score\")\n",
    "voting_ensemble.predict(\"this is a data item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any unified model, also a `VotingEnsemble` can be saved to a single file that includes all the other models combined in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:11:15.211513Z",
     "start_time": "2018-11-19T13:10:50.244191Z"
    }
   },
   "outputs": [],
   "source": [
    "voting_ensemble_path = voting_ensemble.save(os.path.join(output_path,\"voting_ensemble_model.model.zip\"))\n",
    "voting_ensemble = UnifiedModel.load(voting_ensemble_path)\n",
    "voting_ensemble.predict(\"this is a data item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "After training our models, we want to have a closer look into how those models actually perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every unified model provides an `evaluate(test_data, target_predictions)` method It will calculate common metrics for the given model type. For example, for text classification it returns micro/macro precision, recall, and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:11:30.118725Z",
     "start_time": "2018-11-19T13:11:15.216867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select a model\n",
    "selected_model = fasttext_model\n",
    "\n",
    "# Evaluate with test data\n",
    "print(\"Evaluating \"+ str(selected_model))\n",
    "metrics, label_scores = selected_model.evaluate(\n",
    "    test_df['data'].tolist(), test_df['topic'].tolist(), per_label=True)\n",
    "\n",
    "# model metrics\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get evaluation statistics per label (precision, recall, f1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:11:31.144584Z",
     "start_time": "2018-11-19T13:11:30.122970Z"
    }
   },
   "outputs": [],
   "source": [
    "label_scores.style.background_gradient(cmap='BuGn', low=0.1, high=0.8, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare mutlitple models\n",
    "With the `compare_models` method you can evaluate and compare a collection of unified models on the same test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:12:36.035247Z",
     "start_time": "2018-11-19T13:11:31.149728Z"
    }
   },
   "outputs": [],
   "source": [
    "from unified_model import evaluation_utils\n",
    "\n",
    "evaluation_utils.compare_models(\n",
    "    [sklearn_model, fasttext_model, voting_ensemble], \n",
    "    test_df['data'].tolist(), test_df['topic'].tolist(), styled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "- [Lab Client Tutorial](./tutorials/lab-client-tutorial.ipynb): Learn how to connect to ML Lab and run your experiments.\n",
    "- [Experiment Template](../templates/experiment-template.ipynb): Start your own high-quality reusable experiment notebook with this template.\n",
    "- [Introduction to Pandas](./pandas-tutorial.ipynb): Introduction to the data structures and functionalities of the pandas library.\n",
    "- [Introduction to Numpy](./numpy-tutorial.ipynb): Introduction to datatypes, arrays, and mathematical operations of the numpy library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
